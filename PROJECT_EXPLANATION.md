# ğŸ“š Project Explanation: Multi-Agent RAG Research System

## Table of Contents
1. [System Overview](#system-overview)
2. [Core Concepts](#core-concepts)
3. [Architecture Deep Dive](#architecture-deep-dive)
4. [Data Flow Walkthrough](#data-flow-walkthrough)
5. [Implementation Details](#implementation-details)
6. [Design Decisions & Trade-offs](#design-decisions--trade-offs)
7. [Interview Preparation](#interview-preparation)

---

## System Overview

### What Does This Project Do?

This project is an **automated research assistant** that:
1. Takes a research question from a user
2. Searches through a knowledge base of documents
3. Uses multiple AI agents to synthesize, verify, and format the answer
4. Produces a publication-ready research brief

### Why Is This Important?

This project demonstrates mastery of two cutting-edge AI patterns:

1. **RAG (Retrieval-Augmented Generation)**: Instead of relying solely on an LLM's training data, we provide it with relevant context from our own documents. This grounds the AI's responses in verifiable facts.

2. **Multi-Agent Systems**: Instead of one prompt doing everything, we use specialized agents that each excel at one task. This produces higher quality outputs through division of labor.

---

## Core Concepts

### What is RAG?

**RAG = Retrieval-Augmented Generation**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     TRADITIONAL LLM                             â”‚
â”‚                                                                  â”‚
â”‚   Question â”€â”€â”€â”€â–º LLM â”€â”€â”€â”€â–º Answer (from training data)          â”‚
â”‚                                                                  â”‚
â”‚   Problem: LLM may hallucinate or have outdated information     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          RAG                                     â”‚
â”‚                                                                  â”‚
â”‚   Question â”€â”€â”€â”€â–º Search Knowledge Base â”€â”€â”€â”€â–º Relevant Docs      â”‚
â”‚                                    â”‚                             â”‚
â”‚                                    â–¼                             â”‚
â”‚                         LLM + Context â”€â”€â”€â”€â–º Grounded Answer     â”‚
â”‚                                                                  â”‚
â”‚   Benefit: LLM answers are grounded in verifiable documents     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insight**: RAG separates *knowledge storage* from *language generation*. The vector database stores knowledge; the LLM synthesizes answers.

### What are Embeddings?

Embeddings are dense vector representations of text that capture semantic meaning.

```
"The cat sat on the mat"  â”€â”€â”€â”€â–º  [0.2, -0.5, 0.8, ..., 0.1]  (384 dimensions)
"A feline rested on a rug" â”€â”€â”€â–º  [0.21, -0.48, 0.79, ..., 0.12]  (similar!)
"Quantum physics equation" â”€â”€â”€â–º  [-0.7, 0.3, -0.1, ..., 0.9]  (different!)
```

**Why It Matters**: Similar meanings = similar vectors. This enables semantic search (finding relevant content even with different words).

### What is a Vector Database?

A specialized database optimized for similarity search:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     VECTOR DATABASE (ChromaDB)                  â”‚
â”‚                                                                  â”‚
â”‚   Document 1 â”€â”€â–º Embedding 1 â”€â”€â”                                â”‚
â”‚   Document 2 â”€â”€â–º Embedding 2 â”€â”€â”¼â”€â”€â–º Indexed for fast search     â”‚
â”‚   Document 3 â”€â”€â–º Embedding 3 â”€â”€â”˜                                â”‚
â”‚                                                                  â”‚
â”‚   Query: "AI risks" â”€â”€â–º Query Embedding                         â”‚
â”‚                              â”‚                                   â”‚
â”‚                              â–¼                                   â”‚
â”‚                     Find nearest neighbors                       â”‚
â”‚                              â”‚                                   â”‚
â”‚                              â–¼                                   â”‚
â”‚                     Return: Doc 2, Doc 3                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What are Multi-Agent Systems?

Instead of one AI doing everything, we create specialized agents:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SINGLE AGENT (Problematic)                   â”‚
â”‚                                                                  â”‚
â”‚   "Research this topic, verify facts, write clearly,            â”‚
â”‚    format for publishing, make it engaging..."                  â”‚
â”‚                                                                  â”‚
â”‚   Problem: Too many responsibilities = lower quality            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    MULTI-AGENT (Better)                         â”‚
â”‚                                                                  â”‚
â”‚   Researcher: "Find relevant information" â”€â”€â–º Output 1         â”‚
â”‚   Fact-Checker: "Verify these claims" â”€â”€â”€â”€â”€â”€â–º Output 2         â”‚
â”‚   Editor: "Polish this content" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Output 3         â”‚
â”‚   Publisher: "Format for platforms" â”€â”€â”€â”€â”€â”€â”€â”€â–º Final Output     â”‚
â”‚                                                                  â”‚
â”‚   Benefit: Each agent focuses on one thing = higher quality     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Architecture Deep Dive

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SYSTEM ARCHITECTURE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    CONFIGURATION LAYER                              â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚  .env â”€â”€â–º settings.py â”€â”€â–º All Components                           â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚  Purpose: Centralized, secure configuration management              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    DATA LAYER (RAG Pipeline)                        â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚ â”‚
â”‚  â”‚  â”‚ ingest.pyâ”‚ â”€â”€â”€â–º â”‚ ChromaDB â”‚ â—„â”€â”€â”€ â”‚retriever â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  (Write) â”‚      â”‚ (Store)  â”‚      â”‚  (Read)  â”‚                  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚  Purpose: Document storage, embedding, and retrieval                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    PROCESSING LAYER (Agents)                        â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚  agents_rag.py â”€â”€â–º Agent Definitions                                â”‚ â”‚
â”‚  â”‚  orchestrator.py â”€â”€â–º Workflow Coordination                          â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚  Purpose: AI agent definitions and execution flow                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    INTERFACE LAYER                                  â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚  main.py â”€â”€â–º CLI Interface â”€â”€â–º User                                 â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚  Purpose: User interaction and entry point                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### File Responsibilities

| File | Layer | Single Responsibility |
|------|-------|----------------------|
| `configs/settings.py` | Configuration | Load and validate environment variables |
| `rag/ingest.py` | Data | Transform documents into vector embeddings |
| `rag/retriever.py` | Data | Search vector database for relevant chunks |
| `crew/agents_rag.py` | Processing | Define specialized AI agents |
| `crew/orchestrator.py` | Processing | Coordinate agent workflow |
| `main.py` | Interface | Handle user interaction |

---

## Data Flow Walkthrough

### Phase 1: Ingestion (One-time Setup)

```
Step 1: Load Documents
   data/document1.txt â”€â”€â”€â”€â”
   data/document2.txt â”€â”€â”€â”€â”¼â”€â”€â”€â”€â–º Raw text loaded into memory
   data/document3.txt â”€â”€â”€â”€â”˜

Step 2: Chunking
   "This is a long document about AI..."
                â”‚
                â–¼
   ["This is a long", "long document about", "about AI..."]
   
   Why? LLMs have context limits. Smaller chunks = more precise retrieval.

Step 3: Embedding
   "This is a long" â”€â”€â–º [0.2, -0.1, 0.8, ...]  (384-dim vector)
   "long document"  â”€â”€â–º [0.3, -0.2, 0.7, ...]  (384-dim vector)
   
   Why? Vectors enable similarity search.

Step 4: Storage
   Chunks + Embeddings + Metadata â”€â”€â–º ChromaDB (persistent storage)
```

### Phase 2: Retrieval (Every Query)

```
Step 1: User Query
   "What are the risks of AI?"

Step 2: Embed Query
   "What are the risks of AI?" â”€â”€â–º [0.4, -0.3, 0.6, ...]

Step 3: Similarity Search
   Find k=5 chunks closest to query embedding
   
   Algorithm: Approximate Nearest Neighbor (HNSW)
   Complexity: O(log n) - very fast!

Step 4: Return Context
   [
     {text: "AI poses risks such as...", source: "doc1.txt"},
     {text: "Key dangers include...", source: "doc2.txt"},
     ...
   ]
```

### Phase 3: Agent Processing (Sequential)

```
Input: User query + Retrieved context

Agent 1: Researcher
   Input:  Query + Context chunks
   Output: Synthesized findings with citations
   
Agent 2: Fact-Checker  
   Input:  Researcher's output + Original context
   Output: Verified claims with source attribution
   
Agent 3: Editor
   Input:  Verified content
   Output: Polished prose (2-3 paragraphs + bullet points)
   
Agent 4: Publisher
   Input:  Edited content
   Output: JSON with Markdown document + LinkedIn post

Final: Save to artifacts/ folder
```

---

## Implementation Details

### Chunking Strategy

We use **sliding window chunking**:

```python
def semantic_chunking(text, chunk_size=500, overlap=100):
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunk = text[i:i + chunk_size]
        chunks.append(chunk)
    return chunks
```

**Parameters**:
- `chunk_size=500`: Characters per chunk (balance between context and precision)
- `overlap=100`: Overlap between chunks (prevents information loss at boundaries)

**Trade-offs**:
| Chunk Size | Pros | Cons |
|------------|------|------|
| Small (100) | Precise retrieval | May lose context |
| Medium (500) | Good balance | Standard choice |
| Large (1000) | More context | Less precise |

### Embedding Model Choice

We use `all-MiniLM-L6-v2`:

| Factor | Value | Reason |
|--------|-------|--------|
| Dimensions | 384 | Compact but expressive |
| Speed | Fast | ~14,000 sentences/second |
| Quality | Good | Top-10 on MTEB benchmark |
| Cost | FREE | Local execution, no API |

### Agent Prompting

Each agent has three key components:

```python
Agent(
    role="...",       # Job title (affects behavior)
    goal="...",       # What to optimize for
    backstory="...",  # Detailed persona and instructions
    llm=llm,          # Language model to use
    verbose=True      # Show agent thinking
)
```

**Why Backstory Matters**: Detailed backstories improve agent performance by providing:
1. Domain expertise context
2. Behavioral guidelines
3. Output format expectations

### Temperature=0

We set `temperature=0` for all agents:

```python
llm = LLM(model="gpt-4o-mini", temperature=0)
```

**Why?**
- `temperature=0`: Deterministic, factual outputs (good for research)
- `temperature=1`: Creative, varied outputs (good for brainstorming)

---

## Design Decisions & Trade-offs

### Decision 1: Local vs API Embeddings

| Option | Pros | Cons |
|--------|------|------|
| **Local (chosen)** | Free, fast, offline | Less powerful |
| API (OpenAI) | Higher quality | Costs money, requires internet |

**Rationale**: For most use cases, local embeddings are sufficient and eliminate ongoing costs.

### Decision 2: Sequential vs Parallel Agents

| Option | Pros | Cons |
|--------|------|------|
| **Sequential (chosen)** | Predictable, each step builds on previous | Slower |
| Parallel | Faster | Output may be inconsistent |

**Rationale**: Research workflow has natural dependencies (can't verify before researching).

### Decision 3: Single Vector DB vs Multiple

| Option | Pros | Cons |
|--------|------|------|
| **Single (chosen)** | Simple, unified search | Can't separate domains |
| Multiple | Domain separation | More complex |

**Rationale**: For this project scale, single database is appropriate.

---

## Interview Preparation

### Common Questions & Answers

**Q: What is RAG and why is it useful?**

A: RAG (Retrieval-Augmented Generation) combines information retrieval with text generation. Instead of relying solely on an LLM's training data, we first search a knowledge base for relevant context, then provide that context to the LLM. This:
- Reduces hallucinations by grounding responses in sources
- Enables use of private/recent data not in training
- Allows citation and verification of claims

**Q: Why use multiple agents instead of one?**

A: Multi-agent systems follow the Single Responsibility Principle. Each agent specializes in one task:
- Researcher: Finding information
- Fact-Checker: Verifying accuracy
- Editor: Improving clarity
- Publisher: Formatting output

This produces higher quality than one agent trying to do everything.

**Q: Explain the embedding process.**

A: Embeddings convert text to dense vectors (arrays of floats) that capture semantic meaning. Similar texts have similar vectors. The process:
1. Tokenize text into subwords
2. Pass through transformer encoder
3. Pool hidden states into fixed-size vector
4. Normalize to unit length

**Q: What is vector similarity search?**

A: Given a query vector, find the k most similar vectors in the database. We use cosine similarity: vectors pointing in similar directions are similar. ChromaDB uses HNSW (Hierarchical Navigable Small World) algorithm for O(log n) approximate search.

**Q: How do you handle large documents?**

A: We chunk documents into smaller pieces (500 chars with 100 char overlap). This ensures:
- Chunks fit in LLM context windows
- Retrieval is precise (not pulling entire documents)
- Overlap prevents losing information at boundaries

**Q: What's the difference between this and fine-tuning?**

A: 
- RAG: Retrieves relevant context at runtime; knowledge is external
- Fine-tuning: Modifies model weights; knowledge is internal

RAG is better when:
- Data changes frequently
- You need citations
- You want to avoid training costs

### Technical Deep-Dive Questions

**Q: Explain the HNSW algorithm.**

A: HNSW (Hierarchical Navigable Small World) is a graph-based algorithm for approximate nearest neighbor search:
1. Build a multi-layer graph of vectors
2. Start search at top layer (sparse)
3. Navigate to nearest neighbor at each layer
4. Move to lower layer (denser) and repeat
5. Returns approximate k-nearest neighbors in O(log n) time

**Q: Why temperature=0?**

A: Temperature controls randomness in LLM outputs. At temperature=0, the model always selects the highest probability token, making outputs deterministic. This is important for:
- Reproducibility (same input = same output)
- Factual accuracy (no creative embellishment)
- Testing and debugging

**Q: How would you improve retrieval quality?**

A: Several approaches:
1. **Hybrid search**: Combine dense (embedding) + sparse (BM25) retrieval
2. **Reranking**: Use a cross-encoder to rerank initial results
3. **Query expansion**: Generate related queries and merge results
4. **Metadata filtering**: Filter by document type, date, etc.
5. **Fine-tuned embeddings**: Train domain-specific embedding model

---

## Summary

This project demonstrates:

1. **RAG Architecture**: Separating knowledge (vector DB) from reasoning (LLM)
2. **Multi-Agent Orchestration**: Specialized agents with single responsibilities
3. **Production Patterns**: Configuration management, error handling, modular design
4. **ML Engineering**: Embeddings, vector search, chunking strategies

The combination of these patterns creates a robust, explainable, and extensible research system.

---

*Document prepared for technical interviews and portfolio presentation.*
